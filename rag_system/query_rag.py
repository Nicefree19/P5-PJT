"""
Gemini File Search RAG System - ì¿¼ë¦¬ ì¸í„°í˜ì´ìŠ¤

ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰(Semantic Search)ì„ ìˆ˜í–‰í•˜ê³ 
ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
"""

import os
import json
from typing import List, Dict, Optional
import google.generativeai as genai
from google.generativeai.types import content_types

# ì„¤ì •
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")

genai.configure(api_key=GEMINI_API_KEY)


class GeminiRAGQuery:
    """Gemini File Searchë¥¼ í™œìš©í•œ RAG ì¿¼ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, store_name: str = "P5_Project_RAG_Store",
                 model_name: str = "gemini-2.0-flash-exp"):
        """
        Args:
            store_name: File Search Store ì´ë¦„
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸
        """
        self.store_name = store_name
        self.model_name = model_name
        self.store = None
        self.model = None
        self._initialize()

    def _initialize(self) -> None:
        """Storeì™€ Model ì´ˆê¸°í™”"""
        # Store ì°¾ê¸°
        stores = genai.list_file_search_stores()
        for store in stores:
            if store.display_name == self.store_name:
                self.store = store
                print(f"âœ… Store ì—°ê²°: {self.store_name}")
                print(f"   Store ID: {store.name}")
                break

        if not self.store:
            raise ValueError(f"Storeë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.store_name}")

        # Model ì´ˆê¸°í™” (File Search Tool í™œì„±í™”)
        self.model = genai.GenerativeModel(
            model_name=self.model_name,
            tools=[
                genai.protos.Tool(
                    file_search=genai.protos.FileSearchTool(
                        file_search_store_name=self.store.name
                    )
                )
            ]
        )
        print(f"âœ… Model ì´ˆê¸°í™”: {self.model_name}")

    def query(self, question: str,
              system_instruction: Optional[str] = None,
              max_output_tokens: int = 2048,
              temperature: float = 0.2) -> Dict:
        """
        RAG ì¿¼ë¦¬ ì‹¤í–‰

        Args:
            question: ì§ˆë¬¸
            system_instruction: ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ (í˜ë¥´ì†Œë‚˜ ë“±)
            max_output_tokens: ìµœëŒ€ ì¶œë ¥ í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„

        Returns:
            ì¿¼ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print(f"\nğŸ” ì§ˆë¬¸: {question}")
            print("=" * 60)

            # ìƒì„± ì„¤ì •
            generation_config = genai.types.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_output_tokens
            )

            # ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ ì„¤ì •
            if system_instruction:
                model_with_instruction = genai.GenerativeModel(
                    model_name=self.model_name,
                    system_instruction=system_instruction,
                    tools=[
                        genai.protos.Tool(
                            file_search=genai.protos.FileSearchTool(
                                file_search_store_name=self.store.name
                            )
                        )
                    ]
                )
                response = model_with_instruction.generate_content(
                    question,
                    generation_config=generation_config
                )
            else:
                response = self.model.generate_content(
                    question,
                    generation_config=generation_config
                )

            # ì‘ë‹µ íŒŒì‹±
            result = self._parse_response(response, question)
            self._print_result(result)

            return result

        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}

    def _parse_response(self, response, question: str) -> Dict:
        """ì‘ë‹µ íŒŒì‹±"""
        result = {
            'question': question,
            'answer': response.text,
            'grounding_metadata': None,
            'citations': []
        }

        # Grounding Metadata ì¶”ì¶œ
        if hasattr(response, 'grounding_metadata'):
            metadata = response.grounding_metadata
            result['grounding_metadata'] = {
                'grounding_support': str(metadata.grounding_support) if hasattr(metadata, 'grounding_support') else None
            }

            # ì¸ìš© ì •ë³´ ì¶”ì¶œ
            if hasattr(metadata, 'grounding_chunks'):
                for chunk in metadata.grounding_chunks:
                    citation = {
                        'text': chunk.grounding_chunk.text if hasattr(chunk, 'grounding_chunk') else None,
                        'source': chunk.grounding_chunk.source if hasattr(chunk, 'grounding_chunk') else None
                    }
                    result['citations'].append(citation)

        return result

    def _print_result(self, result: Dict) -> None:
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸ’¡ ë‹µë³€:")
        print(result['answer'])

        if result['citations']:
            print(f"\nğŸ“š ì¸ìš© ì¶œì²˜ ({len(result['citations'])}ê°œ):")
            for idx, citation in enumerate(result['citations'][:5], 1):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                print(f"\n[{idx}] {citation.get('source', 'Unknown')}")
                if citation.get('text'):
                    text_preview = citation['text'][:200] + "..." if len(citation['text']) > 200 else citation['text']
                    print(f"    {text_preview}")

    def batch_query(self, questions: List[str],
                    system_instruction: Optional[str] = None) -> List[Dict]:
        """
        ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬

        Args:
            questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            system_instruction: ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­

        Returns:
            ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        for idx, question in enumerate(questions, 1):
            print(f"\n{'='*60}")
            print(f"ì§ˆë¬¸ {idx}/{len(questions)}")
            result = self.query(question, system_instruction)
            results.append(result)

        return results

    def save_results(self, results: List[Dict], output_file: str) -> None:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")


class P5ProjectRAG(GeminiRAGQuery):
    """P5 í”„ë¡œì íŠ¸ íŠ¹í™” RAG ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self):
        # P5 í”„ë¡œì íŠ¸ ì „ìš© ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­
        self.system_instruction = """
ë‹¹ì‹ ì€ P5 ë³µí•©ë™ êµ¬ì¡° í”„ë¡œì íŠ¸ì˜ ì „ë¬¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- PSRC (í”„ë¦¬ìºìŠ¤íŠ¸ ì² ê·¼ ì½˜í¬ë¦¬íŠ¸ ê¸°ë‘¥)
- HMB (í•˜í”„ ìŠ¬ë˜ë¸Œ ë³´)
- PC (í”„ë¦¬ìºìŠ¤íŠ¸ ì½˜í¬ë¦¬íŠ¸ ê±°ë”)
- Steel (ì² ê³¨ ì½”ì–´)

**ì£¼ìš” ì„ë¬´:**
1. í”„ë¡œì íŠ¸ ë¬¸ì„œì—ì„œ ê³µë²•ì  ì •ë³´ ê²€ìƒ‰
2. ì„¤ê³„ ë³€ê²½, Shop Drawing ì´ìŠˆ ë¶„ì„
3. ì ‘í•©ë¶€ ê°„ì„­ ë° ë¦¬ìŠ¤í¬ í‰ê°€
4. ì´í•´ê´€ê³„ìë³„ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ë‚´ìš© ì¶”ì 

**ì‘ë‹µ ì›ì¹™:**
- í•­ìƒ ë¬¸ì„œì˜ ì •í™•í•œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
- ì¶œì²˜ë¥¼ ëª…í™•íˆ ì œì‹œ
- ê³µë²•ì  ë¦¬ìŠ¤í¬ëŠ” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…
- ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰
"""
        super().__init__(store_name="P5_Project_RAG_Store")

    def search_by_keyword(self, keyword: str) -> Dict:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        question = f"í”„ë¡œì íŠ¸ ë¬¸ì„œì—ì„œ '{keyword}'ì™€ ê´€ë ¨ëœ ëª¨ë“  ë‚´ìš©ì„ ì°¾ì•„ì£¼ì„¸ìš”. ê´€ë ¨ íŒŒì¼ëª…ê³¼ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ í¬í•¨í•´ì£¼ì„¸ìš”."
        return self.query(question, system_instruction=self.system_instruction)

    def search_by_stakeholder(self, stakeholder: str) -> Dict:
        """ì´í•´ê´€ê³„ìë³„ ê²€ìƒ‰"""
        question = f"{stakeholder}ì™€ ê´€ë ¨ëœ ëª¨ë“  ì»¤ë®¤ë‹ˆì¼€ì´ì…˜, ì´ìŠˆ, ê²°ì • ì‚¬í•­ì„ ìš”ì•½í•´ì£¼ì„¸ìš”."
        return self.query(question, system_instruction=self.system_instruction)

    def search_issues(self, issue_type: str = "ì ‘í•©ë¶€") -> Dict:
        """ì´ìŠˆ ìœ í˜•ë³„ ê²€ìƒ‰"""
        question = f"{issue_type} ê´€ë ¨ ì´ìŠˆë¥¼ ëª¨ë‘ ì°¾ì•„ì„œ ê¸´ê¸‰ë„ ìˆœìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."
        return self.query(question, system_instruction=self.system_instruction)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description='Gemini File Search RAG ì‹œìŠ¤í…œ - ì¿¼ë¦¬')
    parser.add_argument('--store-name', type=str, default='P5_Project_RAG_Store',
                        help='File Search Store ì´ë¦„')
    parser.add_argument('--model', type=str, default='gemini-2.0-flash-exp',
                        help='ì‚¬ìš©í•  Gemini ëª¨ë¸')
    parser.add_argument('--question', type=str, help='ì§ˆë¬¸')
    parser.add_argument('--batch', type=str, help='ë°°ì¹˜ ì§ˆë¬¸ JSON íŒŒì¼')
    parser.add_argument('--output', type=str, default='query_results.json',
                        help='ê²°ê³¼ ì €ì¥ íŒŒì¼')

    args = parser.parse_args()

    # P5 í”„ë¡œì íŠ¸ íŠ¹í™” RAG ì‚¬ìš©
    rag = P5ProjectRAG()

    if args.question:
        # ë‹¨ì¼ ì§ˆë¬¸
        result = rag.query(args.question, system_instruction=rag.system_instruction)
        rag.save_results([result], args.output)

    elif args.batch:
        # ë°°ì¹˜ ì§ˆë¬¸
        with open(args.batch, 'r', encoding='utf-8') as f:
            questions = json.load(f)
        results = rag.batch_query(questions, system_instruction=rag.system_instruction)
        rag.save_results(results, args.output)

    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        print("ğŸ¤– P5 í”„ë¡œì íŠ¸ RAG ì‹œìŠ¤í…œ")
        print("=" * 60)
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥")
        print()

        while True:
            question = input("ğŸ’¬ ì§ˆë¬¸: ").strip()
            if question.lower() in ['exit', 'quit', 'ì¢…ë£Œ']:
                break
            if question:
                rag.query(question, system_instruction=rag.system_instruction)


if __name__ == '__main__':
    main()
